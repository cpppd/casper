{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code of CNN Casper\n",
    "\n",
    "- The program can implement the LeNet-5, VGG-16 as ConvNet, The fully connective layer can be replaced by a casper tower or a casper layer without cascading the hidden neurons. \n",
    "\n",
    "- Datasets: EMNIST or MNIST. \n",
    " \n",
    "- The codes are modified from lab4.1 and the example code of casper.py. Reference is inclued in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    # defaults\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000    \n",
    "    lr = 0.01 # for SGD\n",
    "    momentum = 0.5 # for SGD\n",
    "    no_cuda = False\n",
    "    seed = 1\n",
    "    log_interval = 500    \n",
    "    \n",
    "    \"\"\"paras for net structure\"\"\"\n",
    "    LeNet5 = True  # True for input->cnn->fc/tower->output, False for input->fc/tower->output\n",
    "    vgg16 = False  # True for input->cnn->fc/tower->output, False for input->fc/tower->output\n",
    "    minimal_net = True  # True for input->output, False for input->hidden->output\n",
    "    add_fc = True  # if True, fc hidden nuerons will be added, Casper layer without cascading the hidden neurons. \n",
    "    add_tower = True  # if True, tower will be added \n",
    "    \"\"\"datasets\"\"\"\n",
    "    EMNIST = True\n",
    "    MNIST = False\n",
    "    \n",
    "    num_fc_0 = 800  # number of fixed fc hidden neurons of initial net, will not increase as the stage grows \n",
    "    num_fc = 5  # number of hidden neurons of casper fc added at one time, Casper layer without cascading the hidden neurons. \n",
    "    num_hidden = 5  # number of hidden neurons of Casper tower added at a time\n",
    "    \n",
    "    \"\"\"initial learning rate (Rprop/Adadelta/Adam)\"\"\"\n",
    "    # Lr_in_before = 0.01  # hidden in before for Rprop/Adadelta\n",
    "    Lr_in_before = 0.001  # hidden in before for Adam, default 0.001\n",
    "    Lr_out_before = 0.001  # hidden out before\n",
    "    Lr_in_after = 0.0005  # hidden in after\n",
    "    Lr_out_after = 0.0005  # hidden out after\n",
    "    \n",
    "    weight_decay_before = 1e-5\n",
    "    weight_decay_after = 2e-5\n",
    "    drop_out_rate = 0\n",
    "    \n",
    "    epochs = 10\n",
    "    stage = 3 \n",
    "    if minimal_net and not add_tower and not add_fc:\n",
    "        stage = 1 # if minimal_net, keep it 1\n",
    "    \"\"\"theshold for adding nuerons by correlation\"\"\"\n",
    "    num_not_decrease = 1800\n",
    "    \n",
    "    \"\"\"input, output\"\"\"\n",
    "    if LeNet5:\n",
    "        num_input = 7*7*64  # size after cnn\n",
    "    elif vgg16:\n",
    "        num_input = 3*3*64  # shrink the size to save time\n",
    "    else:\n",
    "        num_input = 28*28*1  # size of EMNIST/MNIST images\n",
    "    if EMNIST:\n",
    "        number_of_classes = 47  # 47 for EMNIST \n",
    "    elif MNIST:\n",
    "        number_of_classes = 10  # 10 for MNIST\n",
    "        \n",
    "    \"\"\"activation fuction\"\"\"\n",
    "    af = nn.RReLU()  # nn.PReLU(), nn.ReLU(), nn.RReLU()\n",
    "    \n",
    "    \"\"\"batch normalization\"\"\"\n",
    "    bn = True\n",
    "    \"\"\"optimizer\"\"\"\n",
    "    optimiser = \"adam\"  # rprop, adadelta, adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.EMNIST:\n",
    "    # download EMNIST dataset, EMNIST Balanced Dataset is used for classification\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.EMNIST(\n",
    "            root='./data',\n",
    "            split='balanced',\n",
    "            train=True, \n",
    "            download=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.EMNIST(\n",
    "            root='./data', \n",
    "            split='balanced',\n",
    "            train=False, \n",
    "            transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "elif args.MNIST:\n",
    "    # download EMNIST dataset\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            root='./data-MNIST',\n",
    "    #         split='balanced',\n",
    "            train=True, \n",
    "            download=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            root='./data-MNIST', \n",
    "    #         split='balanced',\n",
    "            train=False, \n",
    "            transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a casper network \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()        \n",
    "        if args.LeNet5 == True:\n",
    "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "            self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0)\n",
    "            # self.conv2_drop = nn.Dropout2d()\n",
    "            self.zero_pad = nn.ZeroPad2d(2) # 2 for left, right, up, down\n",
    "        elif args.vgg16 == True:  \n",
    "            out = 8 # default 64\n",
    "            self.conv3_64_1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1)\n",
    "            self.conv3_64_2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1)\n",
    "            self.conv3_128_1 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1)\n",
    "            self.conv3_128_2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1)\n",
    "            self.conv3_256_1 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1)\n",
    "            self.conv3_256_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1)\n",
    "            self.conv3_256_3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1)\n",
    "            self.conv3_512_1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "            self.conv3_512_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "            self.conv3_512_3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "            self.conv3_512_4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "            self.conv3_512_5 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "            self.conv3_512_6 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "            # self.conv2_drop = nn.Dropout2d()\n",
    "            self.zero_pad1 = nn.ZeroPad2d(1)\n",
    "            self.zero_pad2 = nn.ZeroPad2d((1,0,1,0)) # left 1, right 0, up 1, down 0\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Define an artificial neuro network. Transform the pics by takeing advantage of Conv2d so that it \n",
    "            can be introduce to the fully connected layer\n",
    "            \"\"\"\n",
    "            self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1, stride=1)  \n",
    "        if args.minimal_net:\n",
    "            self.layer = torch.nn.Linear(n_feature, n_output)   # origin layer input->output\n",
    "        else:\n",
    "            self.layer_in = torch.nn.Linear(n_feature, args.num_fc_0)\n",
    "            self.layer_out = torch.nn.Linear(args.num_fc_0, n_output)\n",
    "        self.fc = {}\n",
    "        self.hidden = {}\n",
    "        for i in range(1, args.stage):\n",
    "            if args.add_fc:\n",
    "                self.fc[str(i)+\"_in\"] = torch.nn.Linear(n_feature, args.num_fc)\n",
    "                self.fc[str(i)+\"_out\"] = torch.nn.Linear(args.num_fc, n_output)\n",
    "            if args.add_tower:\n",
    "                self.hidden[str(i) + \"_in\"] = torch.nn.Linear(n_feature+(i-1)*args.num_hidden, args.num_hidden) # the ith hidden neural\n",
    "                self.hidden[str(i) + \"_out\"] = torch.nn.Linear(args.num_hidden, n_output) \n",
    "        self.drop_out = torch.nn.Dropout(p = args.drop_out_rate)\n",
    "        if args.bn:\n",
    "            self.bn = nn.BatchNorm1d(args.num_hidden)  # batch normalization for inputs of tower \n",
    "            # self.bn_fc_0 = nn.BatchNorm1d(args.num_fc_0)\n",
    "            self.bn_fc = nn.BatchNorm1d(args.num_fc)  # batch normalization for inputs of fc\n",
    "\n",
    "    def forward(self, x, stage):\n",
    "\n",
    "        \"\"\"\n",
    "        define the forward pass of the cnn(only for transformation) with a relu activation function for \n",
    "        the hidden layer.\n",
    "        \"\"\"\n",
    "        if args.LeNet5:\n",
    "            x = F.relu(F.max_pool2d(self.conv1(self.zero_pad(x)), 2))\n",
    "            # x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(self.zero_pad(x))), 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2(self.zero_pad(x)), 2))\n",
    "            # print(x.data.cpu().numpy().shape) # check the dimension of x\n",
    "        elif args.vgg16:\n",
    "            # 28*28*1\n",
    "            x = F.relu(F.max_pool2d(self.conv3_64_2(self.zero_pad1(self.conv3_64_1(self.zero_pad1(x)))),2))\n",
    "            # 14*14*out*2\n",
    "            x = F.relu(F.max_pool2d(self.conv3_128_2(self.zero_pad1(self.conv3_128_1(self.zero_pad1(x)))),2))\n",
    "            # 7*7*out*4\n",
    "            x = F.relu(F.max_pool2d(self.conv3_256_3(self.zero_pad1(self.conv3_256_2(self.zero_pad1(self.conv3_256_1(self.zero_pad2(x)))))),2))\n",
    "            # 4*4*out*8\n",
    "            # x = F.relu(F.max_pool2d(self.conv3_512_3(self.zero_pad1(self.conv3_512_2(self.zero_pad1(self.conv3_512_1(self.zero_pad1(x)))))),2))  # without pooling\n",
    "            x = F.relu(self.conv3_512_3(self.zero_pad1(self.conv3_512_2(self.zero_pad1(self.conv3_512_1(self.zero_pad1(x)))))))\n",
    "            # max_pool2d-> 1*1*out*8 / without max_pool2d -> 3*3*out*8\n",
    "            x = F.relu(self.conv3_512_6(self.zero_pad1(self.conv3_512_5(self.zero_pad1(self.conv3_512_4(self.zero_pad1(x)))))))  # without pooling\n",
    "            # 1*1*out*8 \n",
    "        else:\n",
    "            x = args.af(self.conv(x))\n",
    "        x_in = x.view(-1, args.num_input) # for casper net\n",
    "                \n",
    "        xx = {}\n",
    "        node_in = [x_in,]\n",
    "        if stage == 1:\n",
    "            if args.minimal_net:   \n",
    "                x = self.layer(x_in)\n",
    "            else:\n",
    "                # x = rrelu(bn_fc_0(self.layer_in(x_in))) # batch normalization\n",
    "                x = args.af(self.layer_in(x_in)) # F.relu is better than F.rrelu for the layer between input and output\n",
    "                x = self.layer_out(x)\n",
    "        elif stage == 2:\n",
    "            if args.minimal_net:\n",
    "                xx[\"0\"] = self.layer(x_in)  \n",
    "                x = xx[\"0\"]\n",
    "            else:\n",
    "                # xx[\"0\"] = rrelu(bn_fc_0(self.layer_in(x_in))) # batch normalization\n",
    "                xx[\"0\"] = args.af(self.layer_in(x_in))\n",
    "                xx[\"0\"] = self.layer_out(xx[\"0\"])\n",
    "                x = xx[\"0\"]\n",
    "            if args.add_fc:\n",
    "                xx[\"fc_1_in\"] = args.af(self.bn_fc(self.fc[\"1_in\"](x_in)))  # input - fc_hidden\n",
    "                xx[\"fc_1_out\"] = self.fc[\"1_out\"](xx[\"fc_1_in\"]) # fc_hidden - output\n",
    "                x += xx[\"fc_1_out\"]\n",
    "            if args.add_tower:\n",
    "                if args.bn:\n",
    "                    xx[\"1_in\"] = args.af(self.bn(self.hidden[\"1_in\"](x_in)))\n",
    "                else:\n",
    "                    xx[\"1_in\"] = args.af(self.hidden[\"1_in\"](x_in))\n",
    "                xx[\"1_out\"] = self.hidden[\"1_out\"](xx[\"1_in\"])\n",
    "                x += xx[\"1_out\"]\n",
    "            # x = xx[\"0\"] + xx[\"1_out\"] + xx[\"fc_1_out\"]\n",
    "        else:\n",
    "            if args.minimal_net:\n",
    "                xx[\"0\"] = self.layer(x_in)  \n",
    "                x = xx[\"0\"]\n",
    "            else:\n",
    "                # xx[\"0\"] = rrelu(bn_fc_0(self.layer_in(x_in))) # batch normalization\n",
    "                xx[\"0\"] = args.af(self.layer_in(x_in))\n",
    "                xx[\"0\"] = self.layer_out(xx[\"0\"]) \n",
    "                x = xx[\"0\"]\n",
    "            if args.add_fc:\n",
    "                if args.bn:\n",
    "                    xx[\"fc_1_in\"] = args.af(self.bn_fc(self.fc[\"1_in\"](x_in)))  # input - fc_hidden\n",
    "                else:\n",
    "                    xx[\"fc_1_in\"] = args.af(self.fc[\"1_in\"](x_in))\n",
    "                xx[\"fc_1_out\"] = self.fc[\"1_out\"](xx[\"fc_1_in\"]) # fc_hidden - output\n",
    "                x += xx[\"fc_1_out\"]\n",
    "            if args.add_tower:\n",
    "                if args.bn:\n",
    "                    xx[\"1_in\"] = args.af(self.bn(self.hidden[\"1_in\"](x_in)))\n",
    "                else:\n",
    "                    xx[\"1_in\"] = args.af(self.hidden[\"1_in\"](x_in))\n",
    "                xx[\"1_out\"] = self.hidden[\"1_out\"](xx[\"1_in\"])\n",
    "                x += xx[\"1_out\"]\n",
    "            # x = xx[\"0\"] + xx[\"1_out\"] + xx[\"fc_1_out\"]\n",
    "            for i in range(2, stage):   \n",
    "                if args.add_fc:\n",
    "                    # node_in.append(xx[\"fc_\"+str(i-1)+\"_in\"]) \n",
    "                    if args.bn:\n",
    "                        xx[\"fc_\"+str(i)+\"_in\"] = args.af(self.bn_fc(self.fc[str(i)+\"_in\"](x_in)))  # input - fc_hidden\n",
    "                    else:\n",
    "                        xx[\"fc_\"+str(i)+\"_in\"] = args.af(self.fc[str(i)+\"_in\"](x_in))\n",
    "                    xx[\"fc_\"+str(i)+\"_out\"] = self.fc[str(i)+\"_out\"](xx[\"fc_\"+str(i)+\"_in\"]) # fc_hidden - output\n",
    "                    x += xx[\"fc_\"+str(i)+\"_out\"]\n",
    "                if args.add_tower:\n",
    "                    node_in.append(xx[str(i-1)+\"_in\"])\n",
    "                    if args.bn:\n",
    "                        xx[str(i)+\"_in\"] = args.af(self.bn(self.hidden[str(i)+\"_in\"](torch.cat(node_in, 1))))\n",
    "                    else:\n",
    "                        xx[str(i)+\"_in\"] = args.af(self.hidden[str(i)+\"_in\"](torch.cat(node_in, 1)))\n",
    "                    xx[str(i)+\"_out\"] = self.hidden[str(i)+\"_out\"](xx[str(i)+\"_in\"])\n",
    "                    x += xx[str(i)+\"_out\"]\n",
    "                # x += xx[str(i)+\"_out\"] + xx[\"fc_\"+str(i)+\"_out\"]\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "        # return x\n",
    "        \n",
    "# optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(args.num_input, args.number_of_classes)\n",
    "\n",
    "if args.cuda:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = {}\n",
    "if args.LeNet5:\n",
    "    optimiser_conv1 = torch.optim.Adam(net.conv1.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv2 = torch.optim.Adam(net.conv2.parameters(), lr=args.Lr_in_before)\n",
    "elif args.vgg16:\n",
    "    optimiser_conv3_64_1 = torch.optim.Adam(net.conv3_64_1.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_64_2 = torch.optim.Adam(net.conv3_64_2.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_128_1 = torch.optim.Adam(net.conv3_128_1.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_128_2 = torch.optim.Adam(net.conv3_128_2.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_256_1 = torch.optim.Adam(net.conv3_256_1.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_256_2 = torch.optim.Adam(net.conv3_256_2.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_256_3 = torch.optim.Adam(net.conv3_256_3.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_512_1 = torch.optim.Adam(net.conv3_512_1.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_512_2 = torch.optim.Adam(net.conv3_512_2.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_512_3 = torch.optim.Adam(net.conv3_512_3.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_512_4 = torch.optim.Adam(net.conv3_512_4.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_512_5 = torch.optim.Adam(net.conv3_512_5.parameters(), lr=args.Lr_in_before)\n",
    "    optimiser_conv3_512_6 = torch.optim.Adam(net.conv3_512_6.parameters(), lr=args.Lr_in_before)\n",
    "\n",
    "if args.minimal_net:\n",
    "    if args.optimiser == \"rprop\":\n",
    "        optimiser_layer_before = torch.optim.Rprop(net.layer.parameters(), lr=args.Lr_in_before*10)\n",
    "        optimiser_layer_after = torch.optim.Rprop(net.layer.parameters(), lr=args.Lr_in_after*10)\n",
    "    elif args.optimiser == \"adadelta\":\n",
    "        optimiser_layer_before = torch.optim.Adadelta(net.layer.parameters(), lr=args.Lr_in_before*10)\n",
    "        optimiser_layer_after = torch.optim.Adadelta(net.layer.parameters(), lr=args.Lr_in_after*10)\n",
    "    elif args.optimiser == \"adam\":\n",
    "        optimiser_layer_before = torch.optim.Adam(net.layer.parameters(), lr=args.Lr_in_before, weight_decay=args.weight_decay_before)\n",
    "        optimiser_layer_after = torch.optim.Adam(net.layer.parameters(), lr=args.Lr_out_before, weight_decay=args.weight_decay_after)\n",
    "else:\n",
    "    if args.optimiser == \"rprop\":\n",
    "        optimiser_layer_in_before = torch.optim.Rprop(net.layer_in.parameters(), lr=args.Lr_in_before*10)  # input - fc_hidden\n",
    "        optimiser_layer_in_after = torch.optim.Rprop(net.layer_in.parameters(), lr=args.Lr_in_after*10)  # input - fc_hidden\n",
    "        optimiser_layer_out_before = torch.optim.Rprop(net.layer_out.parameters(), lr=args.Lr_out_before*10)  # fc_hidden - output\n",
    "        optimiser_layer_out_after = torch.optim.Rprop(net.layer_out.parameters(), lr=args.Lr_out_after*10)  # fc_hidden - output\n",
    "    elif args.optimiser == \"adadelta\":\n",
    "        optimiser_layer_in_before = torch.optim.Adadelta(net.layer_in.parameters(), lr=args.Lr_in_before*10)  # input - fc_hidden\n",
    "        optimiser_layer_in_after = torch.optim.Adadelta(net.layer_in.parameters(), lr=args.Lr_in_after*10)  # input - fc_hidden\n",
    "        optimiser_layer_out_before = torch.optim.Adadelta(net.layer_out.parameters(), lr=args.Lr_out_before*10)  # fc_hidden - output\n",
    "        optimiser_layer_out_after = torch.optim.Adadelta(net.layer_out.parameters(), lr=args.Lr_out_after*10)  # fc_hidden - output\n",
    "    # optimiser_layer_in_before = torch.optim.Adam(net.layer_in.parameters(), lr=args.Lr_in_before, weight_decay=args.weight_decay_before)  # input - fc_hidden\n",
    "    # optimiser_layer_in_after = torch.optim.Adam(net.layer_in.parameters(), lr=args.Lr_in_after, weight_decay=args.weight_decay_after)  # input - fc_hidden\n",
    "    # optimiser_layer_out_before = torch.optim.Adam(net.layer_out.parameters(), lr=args.Lr_out_before, weight_decay=args.weight_decay_before)  # fc_hidden - output\n",
    "    # optimiser_layer_out_after = torch.optim.Adam(net.layer_out.parameters(), lr=args.Lr_out_after, weight_decay=args.weight_decay_after)  # fc_hidden - output\n",
    "    elif args.optimiser == \"adam\":\n",
    "        optimiser_layer_in_before = torch.optim.Adam(net.layer_in.parameters(), lr=args.Lr_in_before)  # input - fc_hidden\n",
    "        optimiser_layer_in_after = torch.optim.Adam(net.layer_in.parameters(), lr=args.Lr_in_after)  # input - fc_hidden\n",
    "        optimiser_layer_out_before = torch.optim.Adam(net.layer_out.parameters(), lr=args.Lr_out_before)  # fc_hidden - output\n",
    "        optimiser_layer_out_after = torch.optim.Adam(net.layer_out.parameters(), lr=args.Lr_out_after)  # fc_hidden - output\n",
    "    # optimiser = torch.optim.Adam(net.parameters(), lr=args.Lr_in_before)\n",
    "    \"\"\"SDG\"\"\"\n",
    "    # optimiser_layer_in_before = optim.SGD(net.layer_in.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    # optimiser_layer_in_after = optim.SGD(net.layer_in.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    # optimiser_layer_out_before = optim.SGD(net.layer_out.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    # optimiser_layer_out_after = optim.SGD(net.layer_out.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "for i in range(1, args.stage):\n",
    "    \"\"\"Rprop\"\"\"\n",
    "    # optimiser[str(i)+\"_in_before\"] = torch.optim.Rprop(net.hidden[str(i)+\"_in\"].parameters(), lr=args.Lr_in_before)\n",
    "    # optimiser[str(i) + \"_in_after\"] = torch.optim.Rprop(net.hidden[str(i)+\"_in\"].parameters(), lr=args.Lr_in_after)\n",
    "    # optimiser[str(i) + \"_out_before\"] = torch.optim.Rprop(net.hidden[str(i) + \"_out\"].parameters(), lr=args.Lr_out_before)\n",
    "    # optimiser[str(i) + \"_out_after\"] = torch.optim.Rprop(net.hidden[str(i) + \"_out\"].parameters(), lr=args.Lr_out_after)\n",
    "    # adaptive according to weights\n",
    "#     optimiser[str(i)+\"_in_before\"] = torch.optim.Rprop(net.hidden[str(i)+\"_in\"].parameters(), lr=args.L1*np.mean([abs(w) for w in net.hidden[str(i) + \"_in\"].weight.data.numpy()]))\n",
    "#     optimiser[str(i) + \"_in_after\"] = torch.optim.Rprop(net.hidden[str(i) + \"_in\"].parameters(), lr=args.L3*np.mean([abs(w) for w in net.hidden[str(i) + \"_in\"].weight.data.numpy()]))\n",
    "#     optimiser[str(i) + \"_out_before\"] = torch.optim.Rprop(net.hidden[str(i) + \"_out\"].parameters(), lr=args.L2*np.mean([abs(w) for w in net.hidden[str(i) + \"_out\"].weight.data.numpy()]))\n",
    "#     optimiser[str(i) + \"_out_after\"] = torch.optim.Rprop(net.hidden[str(i) + \"_out\"].parameters(), lr=args.L4*np.mean([abs(w) for w in net.hidden[str(i) + \"_out\"].weight.data.numpy()])) \n",
    "    \"\"\"Adadelta\"\"\"\n",
    "#     optimiser[str(i)+\"_in_before\"] = torch.optim.Adadelta(net.hidden[str(i)+\"_in\"].parameters(), lr=args.L1)\n",
    "#     optimiser[str(i) + \"_in_after\"] = torch.optim.Adadelta(net.hidden[str(i) + \"_in\"].parameters(), lr=args.L3)\n",
    "#     optimiser[str(i) + \"_out_before\"] = torch.optim.Adadelta(net.hidden[str(i) + \"_out\"].parameters(), lr=args.L2)\n",
    "#     optimiser[str(i) + \"_out_after\"] = torch.optim.Adadelta(net.hidden[str(i) + \"_out\"].parameters(), lr=args.L4)\n",
    "    \"\"\" Adam \"\"\"\n",
    "    if args.add_fc: \n",
    "        if args.optimiser == \"rprop\":\n",
    "            optimiser[\"fc_\"+str(i)+\"_in_before\"] = torch.optim.Rprop(net.fc[str(i)+\"_in\"].parameters(), lr=args.Lr_in_before, weight_decay=args.weight_decay_before)  # input - fc_hidden\n",
    "            # optimiser[\"fc_\"+str(i)+\"_in_after\"] = torch.optim.Rprop(net.fc[str(i)+\"_in\"].parameters(), lr=args.Lr_in_after*np.mean([abs(w) for w in net.fc[str(i)+\"_in\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after)  # input - fc_hidden # adaptive according to weights\n",
    "            optimiser[\"fc_\"+str(i)+\"_in_after\"] = torch.optim.Rprop(net.fc[str(i)+\"_in\"].parameters(), lr=args.Lr_in_after, weight_decay=args.weight_decay_after)  # input - fc_hidden     \n",
    "            optimiser[\"fc_\"+str(i)+\"_out_before\"] = torch.optim.Rprop(net.fc[str(i)+\"_out\"].parameters(), lr=args.Lr_out_before, weight_decay=args.weight_decay_before)  # fc_hidden - output\n",
    "            # optimiser[\"fc_\"+str(i)+\"_out_after\"] = torch.optim.Rprop(net.fc[str(i)+\"_out\"].parameters(), lr=args.Lr_out_after*np.mean([abs(w) for w in net.fc[str(i)+\"_out\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after)  # fc_hidden - output # adaptive according to weights\n",
    "            optimiser[\"fc_\"+str(i)+\"_out_after\"] = torch.optim.Rprop(net.fc[str(i)+\"_out\"].parameters(), lr=args.Lr_out_after, weight_decay=args.weight_decay_after)  # fc_hidden - output\n",
    "        elif args.optimiser == \"adadelta\":\n",
    "            optimiser[\"fc_\"+str(i)+\"_in_before\"] = torch.optim.Adadelta(net.fc[str(i)+\"_in\"].parameters(), lr=args.Lr_in_before, weight_decay=args.weight_decay_before)  # input - fc_hidden\n",
    "            # optimiser[\"fc_\"+str(i)+\"_in_after\"] = torch.optim.Adadelta(net.fc[str(i)+\"_in\"].parameters(), lr=args.Lr_in_after*np.mean([abs(w) for w in net.fc[str(i)+\"_in\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after)  # input - fc_hidden # adaptive according to weights\n",
    "            optimiser[\"fc_\"+str(i)+\"_in_after\"] = torch.optim.Adadelta(net.fc[str(i)+\"_in\"].parameters(), lr=args.Lr_in_after, weight_decay=args.weight_decay_after)  # input - fc_hidden     \n",
    "            optimiser[\"fc_\"+str(i)+\"_out_before\"] = torch.optim.Adadelta(net.fc[str(i)+\"_out\"].parameters(), lr=args.Lr_out_before, weight_decay=args.weight_decay_before)  # fc_hidden - output\n",
    "            # optimiser[\"fc_\"+str(i)+\"_out_after\"] = torch.optim.Adadelta(net.fc[str(i)+\"_out\"].parameters(), lr=args.Lr_out_after*np.mean([abs(w) for w in net.fc[str(i)+\"_out\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after)  # fc_hidden - output # adaptive according to weights\n",
    "            optimiser[\"fc_\"+str(i)+\"_out_after\"] = torch.optim.Adadelta(net.fc[str(i)+\"_out\"].parameters(), lr=args.Lr_out_after, weight_decay=args.weight_decay_after)  # fc_hidden - output\n",
    "        elif args.optimiser == \"adam\":\n",
    "            optimiser[\"fc_\"+str(i)+\"_in_before\"] = torch.optim.Adam(net.fc[str(i)+\"_in\"].parameters(), lr=args.Lr_in_before, weight_decay=args.weight_decay_before)  # input - fc_hidden\n",
    "            # optimiser[\"fc_\"+str(i)+\"_in_after\"] = torch.optim.Adam(net.fc[str(i)+\"_in\"].parameters(), lr=args.Lr_in_after*np.mean([abs(w) for w in net.fc[str(i)+\"_in\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after)  # input - fc_hidden # adaptive according to weights\n",
    "            optimiser[\"fc_\"+str(i)+\"_in_after\"] = torch.optim.Adam(net.fc[str(i)+\"_in\"].parameters(), lr=args.Lr_in_after, weight_decay=args.weight_decay_after)  # input - fc_hidden     \n",
    "            optimiser[\"fc_\"+str(i)+\"_out_before\"] = torch.optim.Adam(net.fc[str(i)+\"_out\"].parameters(), lr=args.Lr_out_before, weight_decay=args.weight_decay_before)  # fc_hidden - output\n",
    "            # optimiser[\"fc_\"+str(i)+\"_out_after\"] = torch.optim.Adam(net.fc[str(i)+\"_out\"].parameters(), lr=args.Lr_out_after*np.mean([abs(w) for w in net.fc[str(i)+\"_out\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after)  # fc_hidden - output # adaptive according to weights\n",
    "            optimiser[\"fc_\"+str(i)+\"_out_after\"] = torch.optim.Adam(net.fc[str(i)+\"_out\"].parameters(), lr=args.Lr_out_after, weight_decay=args.weight_decay_after)  # fc_hidden - output\n",
    "    if args.add_tower:\n",
    "        if args.optimiser == \"rprop\":\n",
    "            optimiser[str(i)+\"_in_before\"] = torch.optim.Rprop(net.hidden[str(i)+\"_in\"].parameters(), lr=args.Lr_in_before, weight_decay=args.weight_decay_before)\n",
    "            # optimiser[str(i) + \"_in_after\"] = torch.optim.Rprop(net.hidden[str(i) + \"_in\"].parameters(), lr=args.Lr_in_after*np.mean([abs(w) for w in net.hidden[str(i)+\"_in\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after) # adaptive according to weights\n",
    "            optimiser[str(i) + \"_in_after\"] = torch.optim.Rprop(net.hidden[str(i) + \"_in\"].parameters(), lr=args.Lr_in_after, weight_decay=args.weight_decay_after)\n",
    "            optimiser[str(i) + \"_out_before\"] = torch.optim.Rprop(net.hidden[str(i) + \"_out\"].parameters(), lr=args.Lr_out_before, weight_decay=args.weight_decay_before)\n",
    "            # optimiser[str(i) + \"_out_after\"] = torch.optim.Rprop(net.hidden[str(i) + \"_out\"].parameters(), lr=args.Lr_out_after*np.mean([abs(w) for w in net.hidden[str(i)+\"_in\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after) # adaptive according to weights\n",
    "            optimiser[str(i) + \"_out_after\"] = torch.optim.Rprop(net.hidden[str(i) + \"_out\"].parameters(), lr=args.Lr_out_after, weight_decay=args.weight_decay_after)\n",
    "        elif args.optimiser == \"adadelta\":\n",
    "            optimiser[str(i)+\"_in_before\"] = torch.optim.Adadelta(net.hidden[str(i)+\"_in\"].parameters(), lr=args.Lr_in_before, weight_decay=args.weight_decay_before)\n",
    "            # optimiser[str(i) + \"_in_after\"] = torch.optim.Adadelta(net.hidden[str(i) + \"_in\"].parameters(), lr=args.Lr_in_after*np.mean([abs(w) for w in net.hidden[str(i)+\"_in\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after) # adaptive according to weights\n",
    "            optimiser[str(i) + \"_in_after\"] = torch.optim.Adadelta(net.hidden[str(i) + \"_in\"].parameters(), lr=args.Lr_in_after, weight_decay=args.weight_decay_after)\n",
    "            optimiser[str(i) + \"_out_before\"] = torch.optim.Adadelta(net.hidden[str(i) + \"_out\"].parameters(), lr=args.Lr_out_before, weight_decay=args.weight_decay_before)\n",
    "            # optimiser[str(i) + \"_out_after\"] = torch.optim.Adadelta(net.hidden[str(i) + \"_out\"].parameters(), lr=args.Lr_out_after*np.mean([abs(w) for w in net.hidden[str(i)+\"_in\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after) # adaptive according to weights\n",
    "            optimiser[str(i) + \"_out_after\"] = torch.optim.Adadelta(net.hidden[str(i) + \"_out\"].parameters(), lr=args.Lr_out_after, weight_decay=args.weight_decay_after)\n",
    "        elif args.optimiser == \"adam\":\n",
    "            optimiser[str(i)+\"_in_before\"] = torch.optim.Adam(net.hidden[str(i)+\"_in\"].parameters(), lr=args.Lr_in_before, weight_decay=args.weight_decay_before)\n",
    "            # optimiser[str(i) + \"_in_after\"] = torch.optim.Adam(net.hidden[str(i) + \"_in\"].parameters(), lr=args.Lr_in_after*np.mean([abs(w) for w in net.hidden[str(i)+\"_in\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after) # adaptive according to weights\n",
    "            optimiser[str(i) + \"_in_after\"] = torch.optim.Adam(net.hidden[str(i) + \"_in\"].parameters(), lr=args.Lr_in_after, weight_decay=args.weight_decay_after)\n",
    "            optimiser[str(i) + \"_out_before\"] = torch.optim.Adam(net.hidden[str(i) + \"_out\"].parameters(), lr=args.Lr_out_before, weight_decay=args.weight_decay_before)\n",
    "            # optimiser[str(i) + \"_out_after\"] = torch.optim.Adam(net.hidden[str(i) + \"_out\"].parameters(), lr=args.Lr_out_after*np.mean([abs(w) for w in net.hidden[str(i)+\"_in\"].weight.data.cpu().numpy()]), weight_decay=args.weight_decay_after) # adaptive according to weights\n",
    "            optimiser[str(i) + \"_out_after\"] = torch.optim.Adam(net.hidden[str(i) + \"_out\"].parameters(), lr=args.Lr_out_after, weight_decay=args.weight_decay_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = torch.nn.CrossEntropyLoss() # no need to use softmax\n",
    "loss_func = torch.nn.NLLLoss()\n",
    "all_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    net.train()\n",
    "    stage = 1\n",
    "    num_loss_without_decrease = 0\n",
    "    count = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "            # if num_loss_without_decrease < args.num_not_decrease*(args.stage if args.stage==1 else args.stage*0.3): # check optimal threshold\n",
    "            if num_loss_without_decrease < args.num_not_decrease:\n",
    "                Y_pred = net(data, stage)\n",
    "            elif stage < args.stage:\n",
    "                num_loss_without_decrease = 0\n",
    "                count = 0\n",
    "                stage += 1\n",
    "                print(\"\\t\\tstage\" + str(stage))                \n",
    "                Y_pred = net(data, stage)\n",
    "            elif stage == args.stage:\n",
    "                Y_pred = net(data, stage)\n",
    "                \n",
    "            # Compute loss\n",
    "            # Here we pass Variables containing the predicted and true values of Y,\n",
    "            # and the loss function returns a Variable containing the loss.\n",
    "            loss = loss_func(Y_pred, target)\n",
    "            # loss_num = loss.data[0]\n",
    "            all_losses.append(float(loss.data[0]))\n",
    "            count += 1\n",
    "            if count >= 2:\n",
    "                if all_losses[len(all_losses) - 1] >= all_losses[len(all_losses) - 2]:\n",
    "                    num_loss_without_decrease += 1\n",
    "\n",
    "            # Clear the gradients before running the backward pass.\n",
    "            net.zero_grad()\n",
    "\n",
    "            # Perform backward pass: compute gradients of the loss with respect to\n",
    "            # all the learnable parameters of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimiser makes an update to its parameters\n",
    "            if stage == 1:\n",
    "                if args.LeNet5:\n",
    "                    optimiser_conv1.step()\n",
    "                    optimiser_conv2.step()\n",
    "                elif args.vgg16:\n",
    "                    optimiser_conv3_64_1.step()\n",
    "                    optimiser_conv3_64_2.step()\n",
    "                    optimiser_conv3_128_1.step()\n",
    "                    optimiser_conv3_128_2.step()\n",
    "                    optimiser_conv3_256_1.step()\n",
    "                    optimiser_conv3_256_2.step()\n",
    "                    optimiser_conv3_256_3.step()\n",
    "                    optimiser_conv3_512_1.step()\n",
    "                    optimiser_conv3_512_2.step()\n",
    "                    optimiser_conv3_512_3.step()\n",
    "                    optimiser_conv3_512_4.step()\n",
    "                    optimiser_conv3_512_5.step()\n",
    "                    optimiser_conv3_512_6.step()\n",
    "                \"\"\"minimal net\"\"\"\n",
    "                if args.minimal_net:\n",
    "                    optimiser_layer_before.step()\n",
    "                else:\n",
    "                    optimiser_layer_in_before.step()\n",
    "                    optimiser_layer_out_before.step()\n",
    "                    # optimiser.step()\n",
    "            elif stage == 2:\n",
    "                if args.LeNet5:\n",
    "                    optimiser_conv1.step()\n",
    "                    optimiser_conv2.step()\n",
    "                elif args.vgg16:\n",
    "                    optimiser_conv3_64_1.step()\n",
    "                    optimiser_conv3_64_2.step()\n",
    "                    optimiser_conv3_128_1.step()\n",
    "                    optimiser_conv3_128_2.step()\n",
    "                    optimiser_conv3_256_1.step()\n",
    "                    optimiser_conv3_256_2.step()\n",
    "                    optimiser_conv3_256_3.step()\n",
    "                    optimiser_conv3_512_1.step()\n",
    "                    optimiser_conv3_512_2.step()\n",
    "                    optimiser_conv3_512_3.step()\n",
    "                    optimiser_conv3_512_4.step()\n",
    "                    optimiser_conv3_512_5.step()\n",
    "                    optimiser_conv3_512_6.step()\n",
    "                \"\"\"minimal net\"\"\"\n",
    "                if args.minimal_net:\n",
    "                    optimiser_layer_after.step()\n",
    "                else:\n",
    "                    optimiser_layer_in_after.step()\n",
    "                    optimiser_layer_out_after.step()\n",
    "                \"\"\"\"fc\"\"\" \n",
    "                if args.add_fc:\n",
    "                    optimiser[\"fc_1_out_before\"].step()  # fc_hidden -> output \n",
    "                    optimiser[\"fc_1_in_before\"].step()  # input -> fc_hidden\n",
    "                \"\"\"tower\"\"\"\n",
    "                if args.add_tower:\n",
    "                    optimiser[\"1_out_before\"].step()\n",
    "                    optimiser[\"1_in_before\"].step()\n",
    "            else:\n",
    "                if args.LeNet5:\n",
    "                    optimiser_conv1.step()\n",
    "                    optimiser_conv2.step()\n",
    "                elif args.vgg16:\n",
    "                    optimiser_conv3_64_1.step()\n",
    "                    optimiser_conv3_64_2.step()\n",
    "                    optimiser_conv3_128_1.step()\n",
    "                    optimiser_conv3_128_2.step()\n",
    "                    optimiser_conv3_256_1.step()\n",
    "                    optimiser_conv3_256_2.step()\n",
    "                    optimiser_conv3_256_3.step()\n",
    "                    optimiser_conv3_512_1.step()\n",
    "                    optimiser_conv3_512_2.step()\n",
    "                    optimiser_conv3_512_3.step()\n",
    "                    optimiser_conv3_512_4.step()\n",
    "                    optimiser_conv3_512_5.step()\n",
    "                    optimiser_conv3_512_6.step()\n",
    "                \"\"\"minimal net\"\"\"\n",
    "                if args.minimal_net:\n",
    "                    optimiser_layer_after.step()\n",
    "                else:\n",
    "                    optimiser_layer_in_after.step()\n",
    "                    optimiser_layer_out_after.step()                \n",
    "                for i in range(1, stage-1):\n",
    "                    \"\"\"\"fc\"\"\" \n",
    "                    if args.add_fc:\n",
    "                        optimiser[\"fc_\"+str(i)+\"_out_after\"].step()\n",
    "                        optimiser[\"fc_\"+str(i)+\"_in_after\"].step()\n",
    "                    \"\"\"tower\"\"\"\n",
    "                    if args.add_tower:\n",
    "                        optimiser[str(i)+\"_out_after\"].step()\n",
    "                        optimiser[str(i)+\"_in_after\"].step()\n",
    "                \"\"\"\"fc\"\"\"\n",
    "                if args.add_fc:\n",
    "                    optimiser[\"fc_\"+str(stage-1)+\"_out_before\"].step()\n",
    "                    optimiser[\"fc_\"+str(stage-1)+\"_in_before\"].step()\n",
    "                \"\"\"tower\"\"\"\n",
    "                if args.add_tower:\n",
    "                    optimiser[str(stage-1)+\"_out_before\"].step()\n",
    "                    optimiser[str(stage-1)+\"_in_before\"].step()\n",
    "    \n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "        \n",
    "        \"\"\" test \"\"\"\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            if args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            output = net(data, stage)            \n",
    "            test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "            # test_loss += loss_func(output, target).data[0]\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * float(correct) / len(test_loader.dataset)))\n",
    "        # print(\"hidden_1_in weight\", np.mean([abs(w) for w in net.hidden[\"1_in\"].weight.data.cpu().numpy()[0]]))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python35\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\nC:\\Python35\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/112800 (0.00%)]\tLoss: 3.892305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32000/112800 (28.36%)]\tLoss: 0.406494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [64000/112800 (56.72%)]\tLoss: 0.327005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [96000/112800 (85.08%)]\tLoss: 0.321566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python35\\lib\\site-packages\\ipykernel_launcher.py:155: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\nC:\\Python35\\lib\\site-packages\\ipykernel_launcher.py:157: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4152, Accuracy: 16161/18800 (85.96%)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/112800 (0.00%)]\tLoss: 0.268475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [32000/112800 (28.36%)]\tLoss: 0.613657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [64000/112800 (56.72%)]\tLoss: 0.357380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [96000/112800 (85.08%)]\tLoss: 0.544275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.3746, Accuracy: 16368/18800 (87.06%)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/112800 (0.00%)]\tLoss: 0.416386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstage2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [32000/112800 (28.36%)]\tLoss: 0.414044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [64000/112800 (56.72%)]\tLoss: 0.440910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [96000/112800 (85.08%)]\tLoss: 0.332013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4850, Accuracy: 15827/18800 (84.19%)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/112800 (0.00%)]\tLoss: 0.551021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [32000/112800 (28.36%)]\tLoss: 0.363190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [64000/112800 (56.72%)]\tLoss: 0.283548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [96000/112800 (85.08%)]\tLoss: 0.311442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4617, Accuracy: 16030/18800 (85.27%)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/112800 (0.00%)]\tLoss: 0.558681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstage3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [32000/112800 (28.36%)]\tLoss: 0.558153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [64000/112800 (56.72%)]\tLoss: 0.407263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [96000/112800 (85.08%)]\tLoss: 0.278998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4593, Accuracy: 15988/18800 (85.04%)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/112800 (0.00%)]\tLoss: 0.409829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [32000/112800 (28.36%)]\tLoss: 0.336508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [64000/112800 (56.72%)]\tLoss: 0.314804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [96000/112800 (85.08%)]\tLoss: 0.364757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4743, Accuracy: 15905/18800 (84.60%)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/112800 (0.00%)]\tLoss: 0.272946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [32000/112800 (28.36%)]\tLoss: 0.443285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [64000/112800 (56.72%)]\tLoss: 0.499677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [96000/112800 (85.08%)]\tLoss: 0.289580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4795, Accuracy: 15795/18800 (84.02%)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [0/112800 (0.00%)]\tLoss: 0.341435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [32000/112800 (28.36%)]\tLoss: 0.366412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [64000/112800 (56.72%)]\tLoss: 0.268591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [96000/112800 (85.08%)]\tLoss: 0.526188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4760, Accuracy: 15910/18800 (84.63%)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/112800 (0.00%)]\tLoss: 0.420732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [32000/112800 (28.36%)]\tLoss: 0.259795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [64000/112800 (56.72%)]\tLoss: 0.453295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [96000/112800 (85.08%)]\tLoss: 0.423052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4932, Accuracy: 15846/18800 (84.29%)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/112800 (0.00%)]\tLoss: 0.371705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [32000/112800 (28.36%)]\tLoss: 0.180015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [64000/112800 (56.72%)]\tLoss: 0.349770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [96000/112800 (85.08%)]\tLoss: 0.303909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4979, Accuracy: 15962/18800 (84.90%)\n\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(1, args.epochs + 1):\n",
    "train(args.epochs)\n",
    "#     test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# torch.save(net, 'model_casper.pth')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "# net = torch.load('model_casper.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
